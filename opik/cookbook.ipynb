{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78a98ed",
   "metadata": {},
   "source": [
    "#### OPIK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5a29fd",
   "metadata": {},
   "source": [
    "OPIK tracks the input and ouput of the decorator attached funtions, it is not focused on llm input and output tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a34cbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just here to help. Is there anything specific youâ€™d like to talk about or any questions you have?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from opik import track\n",
    "\n",
    "os.environ[\"OPIK_API_KEY\"] = \"Wl0D3XzgiHODvnEzdi4CZO4oa\" \n",
    "os.environ[\"OPIK_WORKSPACE\"] = \"brotherbhai54-gmail-com\"\n",
    "\n",
    "\n",
    "from openai import OpenAI\n",
    "from opik.integrations.openai import track_openai\n",
    "client_ollama = track_openai(OpenAI(\n",
    "    base_url=\"http://localhost:11434/v1\",  # Ollama's API URL\n",
    "    api_key=\"fake_key\"  # Required by OpenAI SDK, but Ollama doesn't use it\n",
    "))\n",
    "llm =\"qwen2.5:32b\"\n",
    "\n",
    "\n",
    "\n",
    "@track\n",
    "def retrieve_context(input_text):\n",
    "    return [\n",
    "        \"What specific information are you looking for?\",\n",
    "        \"How can I assist you with your interests today?\",\n",
    "        \"Are there any topics you'd like to explore or learn more about?\",\n",
    "    ]\n",
    "\n",
    "@track\n",
    "def generate_response(input_text, context):\n",
    "    full_prompt = f'If the user asks a question that is not specific, use the context to provide a relevant response.\\nContext: {\", \".join(context)}\\nUser: {input_text}\\nAI:'\n",
    "    response = client_ollama.chat.completions.create(\n",
    "        model=llm, messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "context = retrieve_context(\"Hi how are you?\")\n",
    "print(generate_response(\"Hi how are you?\", context))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd15cb6a",
   "metadata": {},
   "source": [
    "#### LOGFIRE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3692ced0",
   "metadata": {},
   "source": [
    "Logfire is used to track the input sent and output generated by the LLM as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "021463a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07:38:30.782 Calling __main__.retrieve_context\n",
      "07:38:30.782 Calling __main__.generate_response\n",
      "07:38:30.784   Chat Completion with 'qwen2.5:32b' [LLM]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Logfire</span> project URL: <a href=\"https://logfire-us.pydantic.dev/brotherbhai54/starter-project1\" target=\"_blank\"><span style=\"color: #008080; text-decoration-color: #008080; text-decoration: underline\">https://logfire-us.pydantic.dev/brotherbhai54/starter-project1</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mLogfire\u001b[0m project URL: \u001b]8;id=131234;https://logfire-us.pydantic.dev/brotherbhai54/starter-project1\u001b\\\u001b[4;36mhttps://logfire-us.pydantic.dev/brotherbhai54/starter-project1\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just here to help. Is there anything specific you would like to talk about or any questions you have?\n"
     ]
    }
   ],
   "source": [
    "import logfire\n",
    "\n",
    "\n",
    "logfire.configure(token=\"pylf_v1_us_6jStfQVb5DLKrQ3ZG3JtQZKNY3jXpJ1mNhrsqrfWvmwB\",)\n",
    "\n",
    "\n",
    "logfire.instrument_openai(client_ollama)  \n",
    "\n",
    "@logfire.instrument()\n",
    "def retrieve_context(input_text):\n",
    "    return [\n",
    "        \"What specific information are you looking for?\",\n",
    "        \"How can I assist you with your interests today?\",\n",
    "        \"Are there any topics you'd like to explore or learn more about?\",\n",
    "    ]\n",
    "\n",
    "@logfire.instrument()\n",
    "def generate_response(input_text, context):\n",
    "    full_prompt = f'If the user asks a question that is not specific, use the context to provide a relevant response.\\nContext: {\", \".join(context)}\\nUser: {input_text}\\nAI:'\n",
    "    response = client_ollama.chat.completions.create(\n",
    "        model=llm, messages=[{\"role\": \"user\", \"content\": full_prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "context = retrieve_context(\"Hi how are you?\")\n",
    "print(generate_response(\"Hi how are you?\", context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b92abf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
