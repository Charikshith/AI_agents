{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "# EDIT: the __import__ line is not needed, leaving it just \n",
    "# so that the comments make sense\n",
    "# __import__('IPython').embed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions call themselves,  \n",
      "Loops within loops, tracing deep,  \n",
      "Endless paths unfold.\n"
     ]
    }
   ],
   "source": [
    "from agents import Agent, Runner\n",
    "from dotenv import  load_dotenv\n",
    "from agents import set_default_openai_api,set_default_openai_key \n",
    "import os\n",
    "\n",
    "load_dotenv(\"D:\\\\Code\\\\AI\\\\.env\")\n",
    "\n",
    "set_default_openai_api(\"chat_completions\")\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "set_default_openai_key(openai_api_key)\n",
    "\n",
    "agent = Agent(name=\"Assistant\", instructions=\"You are a helpful assistant\",model='gpt-4o-mini')\n",
    "\n",
    "result = Runner.run_sync(agent, \"Write a haiku about recursion in programming.\",max_turns=2)\n",
    "print(result.final_output)\n",
    "\n",
    "# Code within the code,\n",
    "# Functions calling themselves,\n",
    "# Infinite loop's dance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Role: system, user, assistant and developer(new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Responses API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(\"D:\\\\Code\\\\AI\\\\.env\")\n",
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Introduction\n",
    "\n",
    "\n",
    "https://platform.openai.com/docs/api-reference/responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Basic text example with the Chat Completions API\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(model='gpt-4o-mini',\n",
    "                                          messages=[{\n",
    "                                              \"role\": \"user\",\n",
    "                                              'content':\"write a 2 liner joke on cars\"\n",
    "                                          }\n",
    "                                              \n",
    "                                          ])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Basic text example with the Responses API\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(model='gpt-4o-mini',\n",
    "                                   input = [\n",
    "                                       {'role':'user','content':'What is present in the image?'},\n",
    "                                       {'role':'user',\n",
    "                                        'content': [{\n",
    "                                            'type': 'input_image',\n",
    "                                            'image_url': \"https://upload.wikimedia.org/wikipedia/commons/3/3b/LeBron_James_Layup_%28Cleveland_vs_Brooklyn_2018%29.jpg\",\n",
    "                                        }] }\n",
    "                                   ])\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Streaming\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "stream = client.responses.create(model='gpt-4o-mini',\n",
    "                                input=\"write a 2 liner joke on cars\",\n",
    "                            stream=True  )\n",
    "\n",
    "# Store chunks in a list\n",
    "\n",
    "text_chunks=[]\n",
    "\n",
    "for event in stream:\n",
    "    if hasattr(event,\"type\") and \"text.delta\" in event.type:\n",
    "        text_chunks.append(event.delta)\n",
    "        print(event.delta,end=\"\",flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Text Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model spec: https://model-spec.openai.com/2025-02-12.html\n",
    "\n",
    "Dashboard: https://platform.openai.com/logs?api=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs can now be a single string or a list of messages.\n",
    "\n",
    "The list of roles can now be:\n",
    "- system\n",
    "- developer\n",
    "- user\n",
    "- assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Introducing instructions\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(model=\"gpt-4o-mini\",\n",
    "                                   instructions=\"Talk like Caption Jack Sparrow.\",\n",
    "                                   input=\" Tell me a 2 liner joke on ships or seas.\")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Which would be similar to:\n",
    "# --------------------------------------------------------------\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"developer\", \"content\": \"Talk like Caption Jack Sparrow.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a 2 liner joke on ships or seas.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the ordering of authority levels. Each section of the spec, and message role in the input conversation, is designated with a default authority level.\n",
    "\n",
    "- Platform: Model Spec \"platform\" sections and system messages\n",
    "- Developer: Model Spec \"developer\" sections and developer messages\n",
    "- User: Model Spec \"user\" sections and user messages\n",
    "- Guideline: Model Spec \"guideline\" sections\n",
    "- No Authority: assistant and tool messages; quoted/untrusted text and multimodal data in other messages\n",
    "\n",
    "\n",
    "https://model-spec.openai.com/2025-02-12.html#chain_of_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# The chain of command (hierarchical instructions)\n",
    "# --------------------------------------------------------------\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Talk like Caption Jack Sparrow.\"},\n",
    "        {\"role\": \"developer\", \"content\": \"don't Talk like Caption Jack Sparrow.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a 2 liner joke on ships or seas.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)  # talks like a pirate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Don't talk like Caption Jack Sparrow.\"},\n",
    "        {\"role\": \"developer\", \"content\": \"Talk like Caption Jack Sparrow.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me a 2 liner joke on ships or seas.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)  # doesn't talk like a pirate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Conversation State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/docs/guides/conversation-state?api-mode=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Orange who?\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Manual conversation state\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=[\n",
    "        {\"role\": \"user\", \"content\": \"knock knock.\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Who's there?\"},\n",
    "        {\"role\": \"user\", \"content\": \"Orange.\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Dynamic conversation state\n",
    "# --------------------------------------------------------------\n",
    "history = [{\"role\": \"user\", \"content\": \"tell me a joke\"}]\n",
    "\n",
    "response = client.responses.create(model=\"gpt-4o-mini\", input=history, store=False)\n",
    "\n",
    "print(response.output_text)\n",
    "\n",
    "\n",
    "# Add the response to the conversation\n",
    "history += [\n",
    "    {\"role\": output.role, \"content\": output.content} for output in response.output\n",
    "]\n",
    "\n",
    "history.append({\"role\": \"user\", \"content\": \"tell me another\"})\n",
    "\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\", input=history, store=False\n",
    ")\n",
    "\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the scarecrow win an award?\n",
      "\n",
      "Because he was outstanding in his field!\n",
      "The joke is funny due to a clever play on words. \"Outstanding\" has a double meaning: it can mean exceptionally good, which implies the scarecrow is impressive, and it can also mean literally standing out in a field, which is what a scarecrow does. This pun creates a humorous twist, as the punchline connects the unexpected meaning with the situation of the scarecrow. The surprise and wordplay are key elements that make the joke work.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# OpenAI APIs for conversation state (default is to store)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    input=\"tell me a joke\",\n",
    "    # store=True It is true by defaults and stored by OpenAI\n",
    "\n",
    ")\n",
    "print(response.output_text)\n",
    "\n",
    "second_response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    previous_response_id=response.id,\n",
    "    input=[{\"role\": \"user\", \"content\": \"explain why this is funny.\"}],\n",
    ")\n",
    "print(second_response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.4 Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(id='fc_67d6e0cde6788191af69774ba611ee0c01a25152cbf2c95a', arguments='{\"to\":\"chakri@yxm.com\",\"subject\":\"Congratulations!\",\"body\":\"You have been selected.\"}', call_id='call_otTLztNAA3VW2lqU6IEgQ4Ss', name='send_email', type='function_call', status='completed')]\n",
      " \n",
      "{\n",
      "  \"id\": \"fc_67d6e0cde6788191af69774ba611ee0c01a25152cbf2c95a\",\n",
      "  \"arguments\": \"{\\\"to\\\":\\\"chakri@yxm.com\\\",\\\"subject\\\":\\\"Congratulations!\\\",\\\"body\\\":\\\"You have been selected.\\\"}\",\n",
      "  \"call_id\": \"call_otTLztNAA3VW2lqU6IEgQ4Ss\",\n",
      "  \"name\": \"send_email\",\n",
      "  \"type\": \"function_call\",\n",
      "  \"status\": \"completed\"\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client=OpenAI()\n",
    "\n",
    "tools =[\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"send_email\",\n",
    "        \"description\": \" Send an email to a given recipient with a subject and message.\",\n",
    "        \"parameters\": {\n",
    "            'type': \"object\",\n",
    "            \"properties\":{\n",
    "            \"to\" : {\"type\": \"string\",\"description\":\"The recipient email address.\"},\n",
    "            \"subject\":{\"type\":\"string\",\"description\":\"Email subject line.\"},\n",
    "            \"body\":{\"type\":\"string\",\"description\":\"Body of the email message.\"},\n",
    "                        },\n",
    "            'required':[\"to\",\"subject\",\"body\"],\n",
    "            \"additionalProperties\":False,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.responses.create(model='gpt-4o-mini',\n",
    "                                   input=\"Can you send an email to chakri@yxm.com saying you are selected.\",\n",
    "                                   tools = tools)\n",
    "\n",
    "print(response.output,end=\"\\n \\n\")\n",
    "print(response.output[0].model_dump_json(indent=2),end='\\n \\n')\n",
    "# print(response.output[1].model_dump_json(indent=2),end='\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResponseFunctionToolCall(id='fc_67d6e0a7f67c819199582056805e7b3002189529e264aefc', arguments='{\"to\":\"chakri@yxm.com\",\"subject\":\"Selection Notification\",\"body\":\"Congratulations! You are selected.\"}', call_id='call_qxefE6GTLD1YMe78rGtt78IT', name='send_email', type='function_call', status='completed'), ResponseFunctionToolCall(id='fc_67d6e0a8b0cc81918bd951a88a0fe78c02189529e264aefc', arguments='{\"to\":\"jay_roy@yxm.com\",\"subject\":\"Selection Notification\",\"body\":\"Congratulations! You are selected.\"}', call_id='call_nqsCuoMy4WLMOQkPTCqoer4l', name='send_email', type='function_call', status='completed')]\n",
      " \n",
      "{\n",
      "  \"id\": \"fc_67d6e0a7f67c819199582056805e7b3002189529e264aefc\",\n",
      "  \"arguments\": \"{\\\"to\\\":\\\"chakri@yxm.com\\\",\\\"subject\\\":\\\"Selection Notification\\\",\\\"body\\\":\\\"Congratulations! You are selected.\\\"}\",\n",
      "  \"call_id\": \"call_qxefE6GTLD1YMe78rGtt78IT\",\n",
      "  \"name\": \"send_email\",\n",
      "  \"type\": \"function_call\",\n",
      "  \"status\": \"completed\"\n",
      "}\n",
      " \n",
      "{\n",
      "  \"id\": \"fc_67d6e0a8b0cc81918bd951a88a0fe78c02189529e264aefc\",\n",
      "  \"arguments\": \"{\\\"to\\\":\\\"jay_roy@yxm.com\\\",\\\"subject\\\":\\\"Selection Notification\\\",\\\"body\\\":\\\"Congratulations! You are selected.\\\"}\",\n",
      "  \"call_id\": \"call_nqsCuoMy4WLMOQkPTCqoer4l\",\n",
      "  \"name\": \"send_email\",\n",
      "  \"type\": \"function_call\",\n",
      "  \"status\": \"completed\"\n",
      "}\n",
      " \n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"send_email\",\n",
    "        \"description\": \"Send an email to a given recipient with a subject and message.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"to\": {\"type\": \"string\", \"description\": \"The recipient email address.\"},\n",
    "                \"subject\": {\"type\": \"string\", \"description\": \"Email subject line.\"},\n",
    "                \"body\": {\"type\": \"string\", \"description\": \"Body of the email message.\"},\n",
    "            },\n",
    "            \"required\": [\"to\", \"subject\", \"body\"],\n",
    "            \"additionalProperties\": False,\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=[{'role':'user','content': \"Can you send an email to chakri@yxm.com and jay_roy@yxm.com saying you are selected.\"}],\n",
    "    tools=tools)\n",
    "\n",
    "print(response.output,end=\"\\n \\n\")\n",
    "print(response.output[0].model_dump_json(indent=2),end='\\n \\n')\n",
    "print(response.output[1].model_dump_json(indent=2),end='\\n \\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.5 Structured Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Using a JSON Schema\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(\n",
    "    model= 'gpt-4o-mini',\n",
    "    input=[\n",
    "        {'role':'system','content':'Extract the event information.'},\n",
    "        {'role':'user','content':'Karthik and Chakri are going to a AI Fair on Sunday.'},\n",
    "    ],\n",
    "    text={\n",
    "        \"format\": {\n",
    "            \"type\":\"json_schema\",\n",
    "            \"name\": \"calender_event\",\n",
    "            \"schema\": {\n",
    "                \"type\":\"object\",\n",
    "                \"properties\":{\n",
    "                    \"name\":{\"type\":\"string\"},\n",
    "                    \"date\":{\"type\":\"string\"},\n",
    "                    \"participants\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\n",
    "                },\n",
    "                \"required\": ['name','date','participants'],\n",
    "                \"additionalProperties\":False,\n",
    "            },\n",
    "            \"strict\": True,\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "event = json.loads(response.output_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'AI Fair', 'date': 'Sunday', 'participants': ['Karthik', 'Chakri']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://news.ycombinator.com/item?id=43336251"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '__main__.CalanderEvent'>\n",
      "{\n",
      "  \"name\": \"AI Fair\",\n",
      "  \"date\": \"Sunday\",\n",
      "  \"participants\": [\n",
      "    \"Karthik\",\n",
      "    \"Chakri\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Using a Pydantic model (and simple response format)\n",
    "# --------------------------------------------------------------\n",
    "class CalanderEvent(BaseModel):\n",
    "    name : str\n",
    "    date : str\n",
    "    participants: List[str]\n",
    "\n",
    "\n",
    "response = client.responses.parse(model='gpt-4o-mini',\n",
    "                                  input='Karthik and Chakri are going to a AI Fair on Sunday.',\n",
    "                                  instructions=\"Extract the event information\",\n",
    "                                  text_format=CalanderEvent)\n",
    "\n",
    "response_model = response.output[0].content[0].parsed\n",
    "\n",
    "print(type(response_model))\n",
    "print(response_model.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.6 Web Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prabhas, born Venkata Satyanarayana Prabhas Raju Uppalapati on October 23, 1979, in Chennai, India, is a prominent actor in the Telugu film industry, commonly known as Tollywood. He hails from a film-oriented family; his father, Uppalapati Surya Narayana Raju, is a renowned film producer in Telugu cinema. Prabhas completed his schooling at DNR School in Bhimavaram and pursued engineering at Sri Chaitanya College in Hyderabad. ([asianlifestylemagazine.com](https://asianlifestylemagazine.com/the-rise-of-prabhas-from-tollywood-star-to-global-icon/?utm_source=openai))\n",
      "\n",
      "He made his acting debut in 2002 with the film \"Eeswar,\" but it was his role in \"Varsham\" (2004) that established him as a leading actor in Tollywood. His career reached new heights with the epic fantasy film series \"Baahubali,\" directed by S.S. Rajamouli. Prabhas portrayed the dual roles of Amarendra Baahubali and Mahendra Baahubali in \"Baahubali: The Beginning\" (2015) and \"Baahubali 2: The Conclusion\" (2017), which were monumental successes both domestically and internationally. ([asianlifestylemagazine.com](https://asianlifestylemagazine.com/the-rise-of-prabhas-from-tollywood-star-to-global-icon/?utm_source=openai))\n",
      "\n",
      "Following \"Baahubali,\" Prabhas starred in \"Saaho\" (2019), an action-thriller, and \"Radhe Shyam\" (2022), a romantic drama. He has an upcoming lineup of films, including \"Salaar\" and \"Adipurush,\" where he portrays Lord Rama in a retelling of the Ramayana. ([nexnews.org](https://nexnews.org/entertainment/news/celebrating-prabhas-the-journey-of-a-tollywood-star-to-global-icon-on-his-birthday?utm_source=openai))\n",
      "\n",
      "Prabhas is known for his humble nature and dedication to his craft, which has endeared him to fans worldwide. His contributions to Indian cinema have solidified his status as a global icon. ([asianlifestylemagazine.com](https://asianlifestylemagazine.com/the-rise-of-prabhas-from-tollywood-star-to-global-icon/?utm_source=openai)) \n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Basic web search\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(model='gpt-4o-mini',\n",
    "                                   tools=[{\n",
    "                                       \"type\":\"web_search_preview\"\n",
    "                                   }],\n",
    "                                   input=\" Who is Prahas from Tollywood ?\")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyderabad, renowned for its rich culinary heritage, offers a plethora of establishments serving exceptional biryani. Here are some of the top-rated biryani restaurants in the city:\n",
      "\n",
      "**[Bawarchi](https://www.google.com/maps/search/Bawarchi%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Located at RTC Cross Road, Bawarchi is famed for its authentic Hyderabadi dum biryani, offering both chicken and mutton varieties that are juicy and flavorful. Note: The original outlet has no branches.\n",
      "\n",
      "**[Paradise](https://www.google.com/maps/search/Paradise%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "A culinary landmark since 1953, Paradise serves a delicately spiced biryani with tender meat, making it a must-visit for both locals and tourists.\n",
      "\n",
      "**[Hotel Shadab](https://www.google.com/maps/search/Hotel+Shadab%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Situated near Charminar, Hotel Shadab is renowned for its spicy biryani, offering both mutton and chicken options, along with other Hyderabadi delicacies.\n",
      "\n",
      "**[Jewel of Nizam](https://www.google.com/maps/search/Jewel+of+Nizam%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Located within The Golkonda Hotel, this fine dining restaurant offers luxurious ambiance and serves exquisite Hyderabadi biryani, including the Kacchi Dum Biryani.\n",
      "\n",
      "**[Cafe Bahar](https://www.google.com/maps/search/Cafe+Bahar%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Known for its economical yet delicious biryani, Cafe Bahar offers generous portions of chicken, mutton, and fish biryani, making it a favorite among locals.\n",
      "\n",
      "**[Chicha's](https://www.google.com/maps/search/Chicha%27s%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "A modern eatery with a homely touch, Chicha's serves classic Hyderabadi biryani and other traditional dishes in a beautifully decorated environment.\n",
      "\n",
      "**[Hotel Nayaab](https://www.google.com/maps/search/Hotel+Nayaab%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Established in 1986 near Charminar, Hotel Nayaab offers award-winning biryani and other Hyderabadi delicacies, making it a must-visit for food enthusiasts.\n",
      "\n",
      "**[Prince Hotel](https://www.google.com/maps/search/Prince+Hotel%2C+Hyderabad%2C+India)**\n",
      "_Hyderabad, India_\n",
      "Known for its affordable and delicious biryani, Prince Hotel also offers popular dishes like Talawa Gosht and Khichda, catering to a diverse palate.\n",
      "\n",
      "Each of these establishments offers a unique take on the traditional Hyderabadi biryani, ensuring a delightful culinary experience. \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://www.google.com/maps/search/Bawarchi%2C+Hyderabad%2C+India'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Basic web search with location\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"web_search_preview\",\n",
    "            \"user_location\": {\n",
    "                \"type\": \"approximate\",\n",
    "                \"country\": \"IN\",\n",
    "                \"city\": \"Hyderabad\",\n",
    "            },\n",
    "        }\n",
    "    ],\n",
    "    input=\"What are the best biryani restaurants around hyderabad?\",\n",
    ")\n",
    "\n",
    "print(response.output_text)\n",
    "response.output[1].content[0].annotations\n",
    "response.output[1].content[0].annotations[0].url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.7 File Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/storage/files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "import textwrap\n",
    "\n",
    "client = OpenAI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Upload a file\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "def create_file(client, file_path):\n",
    "    if file_path.startswith(\"http://\") or file_path.startswith(\"https://\"):\n",
    "        # Download the file content from the URL\n",
    "        response = requests.get(file_path)\n",
    "        file_content = BytesIO(response.content)\n",
    "        file_name = file_path.split(\"/\")[-1]\n",
    "        file_tuple = (file_name, file_content)\n",
    "        result = client.files.create(file=file_tuple, purpose=\"assistants\")\n",
    "    else:\n",
    "        # Handle local file path\n",
    "        with open(file_path, \"rb\") as file_content:\n",
    "            result = client.files.create(file=file_content, purpose=\"assistants\")\n",
    "    print(result.id)\n",
    "    return result.id\n",
    "\n",
    "\n",
    "# Replace with your own file path or URL\n",
    "file_id = create_file(client, \"https://cdn.openai.com/API/docs/deep_research_blog.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/storage/vector_stores\n",
    "\n",
    "Please be aware of costs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Create a vector store\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\n",
    "vector_store = client.vector_stores.create(name=\"knowledge_base\")\n",
    "print(vector_store.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Add a file to the vector store\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "result = client.vector_stores.files.create(\n",
    "    vector_store_id=vector_store.id, file_id=file_id\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Check status\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "result = client.vector_stores.files.list(vector_store_id=vector_store.id)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Use file search\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "\"\"\"\n",
    "At the moment, you can search in only one vector store at a time, \n",
    "so you can include only one vector store ID when calling the file search tool.\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"What is deep research by OpenAI?\",\n",
    "    tools=[{\"type\": \"file_search\", \"vector_store_ids\": [vector_store.id]}],\n",
    ")\n",
    "print(response)\n",
    "print(textwrap.fill(response.output_text, width=80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --------------------------------------------------------------\n",
    "# Limit results\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    input=\"What is deep research by OpenAI?\",\n",
    "    tools=[\n",
    "        {\n",
    "            \"type\": \"file_search\",\n",
    "            \"vector_store_ids\": [vector_store.id],\n",
    "            \"max_num_results\": 2,\n",
    "        }\n",
    "    ],\n",
    "    include=[\"output[*].file_search_call.search_results\"],\n",
    ")\n",
    "print(response.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------\n",
    "# Similarity search\n",
    "# ----------------------ยง---------------------------------------\n",
    "\n",
    "\n",
    "results = client.vector_stores.search(\n",
    "    vector_store_id=vector_store.id,\n",
    "    query=\"What is deep research by OpenAI?\",\n",
    ")\n",
    "\n",
    "print(results.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.8  Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://platform.openai.com/docs/guides/reasoning?api-mode=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Unsupported parameter: 'reasoning.effort' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'reasoning.effort', 'code': 'unsupported_parameter'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      3\u001b[39m client = OpenAI()\n\u001b[32m      6\u001b[39m prompt = \u001b[33m\"\"\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[33mWrite a bash script that takes a matrix represented as a string with \u001b[39m\n\u001b[32m      8\u001b[39m \u001b[33mformat \u001b[39m\u001b[33m'\u001b[39m\u001b[33m[1,2],[3,4],[5,6]\u001b[39m\u001b[33m'\u001b[39m\u001b[33m and prints the transpose in the same format.\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresponses\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt-4o-mini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meffort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmedium\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# <-----\u001b[39;49;00m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(response.output_text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\openai\\_utils\\_utils.py:279\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\openai\\resources\\responses\\responses.py:602\u001b[39m, in \u001b[36mResponses.create\u001b[39m\u001b[34m(self, input, model, include, instructions, max_output_tokens, metadata, parallel_tool_calls, previous_response_id, reasoning, store, stream, temperature, text, tool_choice, tools, top_p, truncation, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33minput\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m    574\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    575\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    600\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m    601\u001b[39m ) -> Response | Stream[ResponseStreamEvent]:\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    603\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/responses\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    608\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    609\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minstructions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minstructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    610\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_output_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_output_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    611\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    612\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    613\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprevious_response_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_response_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    614\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    615\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtruncation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresponse_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResponseCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mResponseStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\openai\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\openai\\_base_client.py:919\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[39m\n\u001b[32m    916\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    917\u001b[39m     retries_taken = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m919\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Code\\ENVS\\llmenv\\Lib\\site-packages\\openai\\_base_client.py:1023\u001b[39m, in \u001b[36mSyncAPIClient._request\u001b[39m\u001b[34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[39m\n\u001b[32m   1020\u001b[39m         err.response.read()\n\u001b[32m   1022\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1023\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_response(\n\u001b[32m   1026\u001b[39m     cast_to=cast_to,\n\u001b[32m   1027\u001b[39m     options=options,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1031\u001b[39m     retries_taken=retries_taken,\n\u001b[32m   1032\u001b[39m )\n",
      "\u001b[31mBadRequestError\u001b[39m: Error code: 400 - {'error': {'message': \"Unsupported parameter: 'reasoning.effort' is not supported with this model.\", 'type': 'invalid_request_error', 'param': 'reasoning.effort', 'code': 'unsupported_parameter'}}"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Write a bash script that takes a matrix represented as a string with \n",
    "format '[1,2],[3,4],[5,6]' and prints the transpose in the same format.\n",
    "\"\"\"\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"o3-mini\",\n",
    "    reasoning={\"effort\": \"medium\"}, # <-----\n",
    "    input=[{\"role\": \"user\", \"content\": prompt}],\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
